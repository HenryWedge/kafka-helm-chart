apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: {{ .Values.name }}
  namespace: {{ .Values.namespace }}
spec:
  entityOperator:
    userOperator:
      watchedNamespace: 'kafka'
      reconciliationIntervalSeconds: 30
      logging:
        type: 'inline'
        loggers:
          {{ range $key, $value := .Values.loggers }}
          "{{ $key }}": '{{ $value }}'
          {{ end }}
      resources:
        requests:
          cpu: {{ .Values.entityOperator.requests.cpu }}
          memory: {{ .Values.entityOperator.requests.memory }}
        limits:
          cpu: {{ .Values.entityOperator.requests.cpu }}
          memory: {{ .Values.entityOperator.requests.memory }}
  kafka:
{{ if .Values.distributeWideOnNodes }}
    template:
      pod:
        affinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 75
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: 'strimzi.io/name'
                      operator: 'In'
                      values:
                        - 'kafka'
                topologyKey: {{ .Values.topologyKey }}
            - weight: 50
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: 'strimzi.io/name'
                      operator: 'In'
                      values:
                        - 'zookeeper'
                topologyKey: {{ .Values.topologyKey }}
{{ end }}
    logging:
      type: 'inline'
      loggers:
        {{ range $key, $value := .Values.loggers }}
        "{{ $key }}": '{{ $value }}'
        {{ end }}
    authorization:
      superUsers: {{ .Values.superUsers }}
      type: 'simple'
    config:
      {{ range $key, $value := .Values.brokerConfig }}
      "{{ $key }}": '{{ $value }}'
      {{ end }}
    jvmOptions: # Kafka broker does never require more than 4 to 6 GB heap
    # Minimal and maximal heap size should be identical to have explainable resources
    # Heap size must not exceed the memory of the pod
    #calculateHeapSize(availableMemory=kafkaSizing.memory, maximumHeapSize=4) +
    #TODO set heap size
    #{
    #  "-XX": {
    #    UseG1GC: true,
    #    MaxGCPauseMillis: 20,
    #    InitiatingHeapOccupancyPercent: 35, #
    #    G1HeapRegionSize: '16M',
    #    MinMetaspaceFreeRatio: 50,
    #    MaxMetaspaceFreeRatio: 80,
    #    ExplicitGCInvokesConcurrent: true,
    #  },
    #  "-D": "javax.net.debug=ssl"
    #},
    listeners:
      - authentication:
          type: 'tls'
        name: 'tls'
        port: 9092
        tls: true
        type: 'internal'
      - name: 'plain'
        port: 9093
        tls: false
        type: 'internal'
      - name: 'external'
        tls: true
        port: 9094
        type: ingress
        authentication:
          type: tls
        configuration:
          bootstrap:
            host: <bootstrapHost>
          brokers:
          {{ range $i := until (int .Values.broker.replicas) }}
            - broker: {{ $i }}
              host: broker-{{ $i }}.{{ .Values.hostname }}
          {{ end }}
          class: {{ .Values.ingressClassName }}
    replicas: {{ .Values.broker.replicas }}
    resources:
      requests:
        cpu: {{ .Values.broker.requests.cpu }}
        memory: {{ .Values.broker.requests.memory }}
      limits:
        cpu: {{ .Values.broker.requests.cpu }}
        memory: {{ .Values.broker.requests.memory }}
    storage:
      type: 'jbod'
      volumes:
        - class: {{ .Values.storageClass }}
          deleteClaim: true
          id: 0
          size: {{ .Values.broker.requests.disk }}
          type: 'persistent-claim'
    metricsConfig:
      type: 'jmxPrometheusExporter'
      valueFrom:
        configMapKeyRef:
          name: 'kafka-metrics-config'
          key: 'kafka-metrics-config.yml'
  zookeeper:
{{ if .Values.distributeWideOnNodes }}
    template:
      pod:
        affinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 75
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: 'strimzi.io/name'
                      operator: 'In'
                      values:
                        - 'kafka'
                topologyKey: {{ .Values.topologyKey }}
            - weight: 50
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: 'strimzi.io/name'
                      operator: 'In'
                      values:
                        - 'zookeeper'
                topologyKey: {{ .Values.topologyKey }}
{{- end}}
    config:
      "autopurge.snapRetainCount": 10
      "autopurge.purgeInterval": 24
    logging:
      type: 'inline'
      loggers:
        {{ range $key, $value := .Values.loggers }}
        "{{ $key }}": '{{ $value }}'
        {{ end }}
    #jvmOptions: calculateHeapSize(zookeeperSizing.memory, 1)
    replicas: {{ .Values.zookeeper.replicas }}
    resources:
      requests:
        cpu: {{ .Values.zookeeper.requests.cpu }}
        memory: {{ .Values.zookeeper.requests.memory }}
      limits:
        cpu: {{ .Values.zookeeper.requests.cpu }}
        memory: {{ .Values.zookeeper.requests.memory }}
    storage:
      class: {{ .Values.storageClass }}
      deleteClaim: true
      size: {{ .Values.zookeeper.requests.disk }}
      type: 'persistent-claim'
    metricsConfig:
      type: 'jmxPrometheusExporter'
      valueFrom:
        configMapKeyRef:
          name: 'kafka-metrics-config'
          key: 'zookeeper-metrics-config.yml'
  kafkaExporter:
    groupRegex: '.*'
    topicRegex: '.*'
    groupExcludeRegex: '^excluded-.*'
    topicExcludeRegex: '^excluded-.*'
    resources:
      requests:
        cpu: '200m'
        memory: '64Mi'
      limits:
        cpu: '500m'
        memory: '128Mi'
    logging: 'debug'
    enableSaramaLogging: true
    template:
      pod:
        securityContext:
          runAsUser: 1000001
          fsGroup: 0
        terminationGracePeriodSeconds: 120
    readinessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
    livenessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5